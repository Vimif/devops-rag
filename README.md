# ğŸ¤– Assistant IA DevOps Auto-hÃ©bergÃ©

SystÃ¨me RAG (Retrieval-Augmented Generation) 100% local pour assister les projets DevOps.

## ğŸ“‹ PrÃ©requis

- **OS**: Ubuntu 24.04 LTS
- **GPU**: NVIDIA RTX 3080 (10GB VRAM)
- **RAM**: 16GB minimum (32GB recommandÃ©)
- **Docker**: InstallÃ© et configurÃ©
- **Stockage**: ~50GB libre

## ğŸš€ Installation rapide

```bash
# 1. Cloner ou copier ce dossier
cd devops-rag

# 2. Rendre le script exÃ©cutable
chmod +x install.sh scripts/*.sh

# 3. Lancer l'installation
./install.sh
```

L'installation:
1. VÃ©rifie les prÃ©requis (Docker, GPU, drivers)
2. Installe NVIDIA Container Toolkit si nÃ©cessaire
3. DÃ©marre les services Docker
4. TÃ©lÃ©charge les modÃ¨les IA (~5GB)

## ğŸ“ Structure du projet

```
devops-rag/
â”œâ”€â”€ docker-compose.yml      # Configuration des services
â”œâ”€â”€ install.sh              # Script d'installation
â”œâ”€â”€ README.md               # Ce fichier
â”œâ”€â”€ config/
â”‚   â””â”€â”€ system-prompt.md    # Prompt systÃ¨me pour l'assistant
â”œâ”€â”€ data/                   # DonnÃ©es persistantes (gÃ©nÃ©rÃ©)
â”‚   â”œâ”€â”€ ollama/             # ModÃ¨les LLM
â”‚   â”œâ”€â”€ qdrant/             # Base vectorielle
â”‚   â””â”€â”€ open-webui/         # Configuration UI
â”œâ”€â”€ docs/                   # ğŸ“„ VOTRE DOCUMENTATION ICI
â”‚   â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ conventions/
â”‚   â”œâ”€â”€ exigences/
â”‚   â””â”€â”€ ...
â””â”€â”€ scripts/
    â”œâ”€â”€ ingest.sh           # Script d'ingestion simple
    â””â”€â”€ ingest_api.py       # Ingestion avancÃ©e via API
```

## ğŸ¯ Utilisation

### 1. AccÃ©der Ã  l'interface

Ouvrez [http://localhost:3000](http://localhost:3000) dans votre navigateur.

### 2. Configuration initiale

1. **CrÃ©er un compte** administrateur
2. **SÃ©lectionner le modÃ¨le**: Settings â†’ Models â†’ `qwen2.5-coder:7b-instruct`
3. **Configurer le system prompt**: Copiez le contenu de `config/system-prompt.md`

### 3. Ajouter votre documentation

**MÃ©thode simple** (via l'interface):
1. Allez dans Settings â†’ Documents
2. Cliquez sur "+" pour uploader vos fichiers
3. Glissez-dÃ©posez depuis le dossier `docs/`

**MÃ©thode avancÃ©e** (via script):
```bash
# Placer vos fichiers dans ./docs/
cp -r /chemin/vers/votre/documentation/* docs/

# Installer les dÃ©pendances Python (une seule fois)
pip install langchain langchain-community langchain-qdrant qdrant-client

# Lancer l'ingestion
python scripts/ingest_api.py

# Forcer la rÃ©indexation complÃ¨te
python scripts/ingest_api.py --force
```

### 4. Poser des questions

Dans l'interface de chat, activez le RAG en cliquant sur l'icÃ´ne ğŸ“ et sÃ©lectionnez vos documents, puis posez vos questions:

- "Comment dÃ©ployer l'application selon notre architecture?"
- "Montre-moi un exemple de pipeline CI/CD conforme Ã  nos conventions"
- "Quelles sont les exigences pour le monitoring?"

## ğŸ”§ Services disponibles

| Service | URL | Description |
|---------|-----|-------------|
| Open WebUI | http://localhost:3000 | Interface de chat |
| Ollama API | http://localhost:11434 | API LLM |
| Qdrant | http://localhost:6333/dashboard | Base vectorielle |

## ğŸ“Š Commandes utiles

```bash
# Voir les logs en temps rÃ©el
docker compose logs -f

# Logs d'un service spÃ©cifique
docker compose logs -f ollama

# RedÃ©marrer les services
docker compose restart

# ArrÃªter les services
docker compose down

# ArrÃªter et supprimer les donnÃ©es
docker compose down -v

# Voir l'utilisation GPU
watch -n 1 nvidia-smi

# Lister les modÃ¨les Ollama installÃ©s
docker exec ollama ollama list

# TÃ©lÃ©charger un nouveau modÃ¨le
docker exec ollama ollama pull <nom-du-modele>

# Tester le modÃ¨le en CLI
docker exec -it ollama ollama run qwen2.5-coder:7b-instruct
```

## ğŸ”„ Mise Ã  jour de la documentation

Quand vous modifiez vos fichiers de documentation:

```bash
# Le script dÃ©tecte automatiquement les fichiers modifiÃ©s
python scripts/ingest_api.py
```

Seuls les fichiers modifiÃ©s seront rÃ©indexÃ©s.

## ğŸ“š Formats de fichiers supportÃ©s

| CatÃ©gorie | Extensions |
|-----------|------------|
| Documentation | `.md`, `.txt`, `.html` |
| Configuration | `.yaml`, `.yml`, `.json`, `.toml`, `.ini` |
| Infrastructure | `.tf`, `.hcl`, `Dockerfile`, `docker-compose.yml` |
| Scripts | `.sh`, `.bash`, `.py`, `Makefile`, `Jenkinsfile` |
| CI/CD | `.gitlab-ci.yml`, `.github/workflows/*.yml` |
| Code | `.py`, `.go`, `.js`, `.ts`, `.java`, `.rs` |

## âš¡ Optimisation pour RTX 3080

La configuration est optimisÃ©e pour 10GB de VRAM:

- **ModÃ¨le principal**: `qwen2.5-coder:7b-instruct` (~5GB VRAM)
- **1 modÃ¨le chargÃ©** Ã  la fois en mÃ©moire
- **2 requÃªtes parallÃ¨les** maximum

Pour tester le modÃ¨le 14B (Ã  la limite):
```bash
docker exec ollama ollama pull qwen2.5-coder:14b-instruct-q4_K_M
```

## ğŸ”’ SÃ©curitÃ©

- âœ… 100% local - aucune donnÃ©e ne quitte votre machine
- âœ… Pas de connexion internet requise aprÃ¨s installation
- âš ï¸ Par dÃ©faut, l'inscription est ouverte - dÃ©sactivez-la aprÃ¨s crÃ©ation du compte admin

Pour dÃ©sactiver l'inscription:
```yaml
# Dans docker-compose.yml, changez:
- ENABLE_SIGNUP=false
```

## ğŸ› DÃ©pannage

### Le GPU n'est pas dÃ©tectÃ©

```bash
# VÃ©rifier les drivers
nvidia-smi

# VÃ©rifier Docker + GPU
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# RedÃ©marrer Docker
sudo systemctl restart docker
```

### Ollama est lent au premier dÃ©marrage

Normal - le modÃ¨le doit Ãªtre chargÃ© en VRAM. Les requÃªtes suivantes seront rapides.

### Erreur "out of memory"

Le modÃ¨le est trop gros. Utilisez un modÃ¨le plus petit:
```bash
docker exec ollama ollama pull qwen2.5-coder:3b-instruct
```

### Open WebUI ne dÃ©marre pas

VÃ©rifiez que Ollama est bien dÃ©marrÃ©:
```bash
docker compose logs ollama
curl http://localhost:11434/
```

## ğŸ“ Personnalisation

### Changer le modÃ¨le par dÃ©faut

Modifiez `docker-compose.yml`:
```yaml
environment:
  - DEFAULT_MODELS=qwen2.5-coder:14b-instruct-q4_K_M
```

### Ajuster le chunking

Modifiez les paramÃ¨tres dans `scripts/ingest_api.py`:
```python
CHUNK_CONFIG = {
    "md": {"chunk_size": 1000, "chunk_overlap": 150, ...},
    ...
}
```

## ğŸ“„ Licence

Ce projet utilise des composants open-source:
- [Ollama](https://ollama.ai/) - MIT License
- [Open WebUI](https://github.com/open-webui/open-webui) - MIT License
- [Qdrant](https://qdrant.tech/) - Apache 2.0
- [Qwen 2.5 Coder](https://huggingface.co/Qwen) - Apache 2.0
